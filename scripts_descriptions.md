### Script Descriptions

1) ### link_construct.py

This script connects to a MySQL database to retrieve media records for a given study, filters them by date, and generates download links for each media file.
It distinguishes between images and videos, automatically creating the appropriate folders for each.

# What it does:

- Queries the database for all publications associated with a given study and date range.
- Generates AWS S3 links for non-Twitter media and uses the original URLs for Twitter media.
- Creates folders named according to media type and study ID (e.g., images_53_10, videos_53_10).
- Saves all retrieved links into text files (aws_links.txt and twitter_links.txt) inside each folder.

# Main configuration:

- study_id: the ID of the study to process (default "53").

- START_DATE and END_DATE: define the date range for media retrieval.

- TWITTER_PLATFORM_ID: identifies Twitter media to keep their original URLs.

# How to use it:
Run the script after setting the correct study ID and date range.
It will automatically:
- Query the database
- Create the necessary folders
- Generate and store all corresponding AWS and Twitter links

#########################################################################################################################

2) ### downloader.py

This script handles the download and preprocessing of images and videos for a given study.
It can be used for both dataset preparation (for training YOLO models) and media analysis (for detection tasks).

# What it does:

- Reads media URLs generated by link_construct.py from aws_links.txt and twitter_links.txt.
- Downloads all images or videos associated with a specific study_id.
- Handles both AWS-hosted and Twitter-hosted files automatically.
- Saves all successfully downloaded files in purpose-specific folders.
- Logs any failed downloads for review or retry.
- Optionally extracts frames from short videos (for dataset creation).

# Main configuration:

study_id: the ID of the study to process.

# purpose:

"dataset" → creates YOLO-ready datasets (extracts frames and handles long videos).

"media_analysis" → downloads videos or images without splitting frames.

frame_threshold: determines whether a video should be split into frames or moved to the videos_to_be_cut folder.

# Functions Overview:

- download_images_for_study(study_id, purpose="dataset"):
Downloads all image URLs for the given study and saves them in the appropriate folder.

- download_and_process_videos(study_id, purpose="dataset", frame_threshold=150):
Downloads all video URLs. If in dataset mode, it extracts frames for short videos and moves long ones to a separate folder.

- retry_failed_downloads(study_id):
Reads the error log and attempts to re-download failed files by trying alternate extensions.

# How to use it:

- Make sure link_construct.py has been executed — this generates the aws_links.txt and twitter_links.txt files.
- Open downloader.py and set the study_id and purpose at the bottom of the file.
- Run the script directly with Python: "python downloader.py"
- It will automatically create the required folders, download media, and log any errors.

############################################################################################################################

3) ### image_extraction.py

This script provides flexible control for extracting frames from videos with customizable modes, strategies, and time ranges.
It is mainly used to process downloaded videos (e.g., from downloader.py) and extract representative frames for dataset preparation or visual inspection.

# What it does:

Reads a specified video file and retrieves basic metadata such as FPS, total frame count, and duration.

Allows users to choose between several frame extraction modes:
- all → extracts every frame.
- percentage → extracts a percentage of frames from the video.
- step → extracts every nth frame.
- time → extracts one frame every x seconds.
Optionally limits extraction to a defined start and end time range within the video.

Supports two selection strategies for sampled frames:
- equal → evenly spaced frames.
- random → randomly chosen frames.

Saves all extracted frames in the target dataset folder with descriptive filenames.
Displays extraction progress and performance summary in the console.

# Main configuration:

- video_path: the full path of the video to process.
- output_folder: the directory where frames will be saved.
- mode: one of the four extraction options (all, percentage, step, time).
- value: depends on the selected mode (percentage value, step size, or time interval).
- strategy: determines how frames are selected (equal or random).
- start_sec / end_sec: define an optional time window for partial extraction.

# Functions Overview:

get_video_info(video_path): retrieves FPS, total frames, and duration of a video.

video_to_frames(...): handles the main extraction logic based on the chosen mode and parameters.

apply_strategy(frames_list, strategy, n_target): applies either equal spacing or random frame sampling.

extract_frames_standard(...): saves selected frames to disk with clear, structured naming.

# How to use it:

- Make sure the target video (for example, one from the videos_to_be_cut folder) exists and the paths are correctly set at the bottom of the script.
- Run the script directly with Python : "python image_extraction.py"
- Follow the on-screen prompts to choose:
   The extraction mode (e.g., percentage or step).
   Whether to apply a time range.
   The frame selection strategy (equal or random).
- Extracted frames will be saved automatically in the configured output folder, with a summary of total frames saved and elapsed time

#############################################################################################################################

4) ### image_uploader.py

This script automates the upload of locally stored images to a Label Studio project using its API.
It supports batch uploading and refreshes the API token automatically during execution.

# What it does:

- Walks through a local folder (e.g., dataset prepared by downloader.py or image_extraction.py) and identifies all image files (.jpg, .jpeg, .png).
- Builds the relative path for each image to match Label Studio’s local file serving requirements.
- Sends a POST request to the Label Studio API to upload each image to the specified project.
- Refreshes the access token automatically using the refresh_access_token() function.
- Stops uploading after reaching a defined maximum number of images (max_uploads).
- Logs upload status and summarizes start and end times.

# Main configuration:

local_folder: path to the folder containing images to upload.

PROJECT_ID: the target Label Studio project ID.

max_uploads: maximum number of images to upload in a single run.

API_TOKEN: the current access token for the Label Studio API (auto-refresh handled).

API_URL: endpoint for creating tasks in Label Studio.

# Pre-requisites:

- Before running the script, ensure the following environment variables are set on terminal:

LABEL_STUDIO_LOCAL_FILES_SERVING_ENABLED = "true"

LABEL_STUDIO_LOCAL_FILES_DOCUMENT_ROOT = full path to the root folder containing images (e.g., "C:\Users\JuniorCHIEMMANUELNGU\Desktop\test")

- Additionally:

Start the Label Studio server with "label-studio start".

Configure the target storage in Label Studio to point to the full path of the folder containing images. (e.g., "C:\Users\JuniorCHIEMMANUELNGU\Desktop\test\dataset_53")

# How to use it:

- Set the environment variables as described above.
- Make sure Label Studio is running and the project exists.
- Open the script and set local_folder and PROJECT_ID to your target paths.
- Run the script with Python : "python image_uploader.py"
- The script will walk through the folder, upload images, and display progress in the console.
- Check the console output for any failed uploads. The script stops automatically when reaching

########################################################################################################################################

5) ### yolo.py

This script trains a YOLO model on a prepared dataset and evaluates it after training.

# What it does:

- Loads a YOLO model (e.g., yolo11n.pt) as the base for fine-tuning.
- Trains the model on a dataset specified in a data.yaml configuration file.
- training parameters such as number of epochs, image size, batch size, and experiment name.
- Validates the trained model on the validation set and outputs evaluation metrics.
- Prints start and end timestamps, as well as total training duration.

# Main configuration:

- model: path to the YOLO base model weights (.pt).
- data.yml: file containing dataset configurations
- epochs: number of training epochs (e.g., 100).
- imgsz: input image size for training (e.g., 640).
- batch: batch size per iteration (e.g., 8).
- name: name of the training run/folder for saving weights and results.

# How to use it:

- Prepare your dataset according to the YOLO structure: separate train and val folders with labeled images.
- Ensure data.yaml points to the correct dataset paths and class names.
- Set the path to the base model (YOLO("yolo11n.pt")).
- Run the script with python: "python yolo.py"
- After training, validation metrics are printed and the best model weights are saved under runs/train/<name>/weights/best.pt.

#############################################################################################################################

6) ### image_detection.py

This script applies a trained YOLO model to detect objects in all images within a specified folder. It saves both annotated images and structured detection outputs in JSON format.

# What it does:

- Loads a YOLO model from specified weights (e.g., best.pt).
- Iterates over all images (.jpg, .jpeg, .png) in a target folder.
- Runs object detection on each image.
- Saves annotated images in a detections_images folder.
- Saves detection results in JSON format in a detection_jsons folder.
- Prints processing time for each image and overall statistics.

# Main configuration:

images_folder: path to the folder containing images to process.

model_weights_path: path to the trained YOLO model weights.

# How to use it:

- Prepare a folder of images you want to run detections on.
- Ensure you have a trained YOLO model weights file.
- Update images_folder and model_weights_path at the bottom of the script.
- Run the script with Python: "python image_detection.py"
- The script will automatically create detections_images and detection_jsons folders alongside the source images and save outputs there.
- At the end, it prints total processing time and average time per image.

7) ### video_analysis_with_tracker.py

This script applies a trained YOLO model to detect objects in all videos within a specified folder. It uses SORT tracking to provide temporal consistency but keeps only YOLO boxes and labels. Only detections above a confidence threshold are considered.

# What it does:

- Loads a YOLO model from specified weights.
- Iterates over all video files (.mp4, .mov, .avi) in a target folder.
- Runs object detection on each frame of every video.
- Filters out detections with confidence below 0.35.
- Applies SORT tracking to maintain temporal consistency of detected objects.
- Saves annotated videos with YOLO boxes and class names only in a detected_videos_with_tracking_fine_tuneV2 folder.
- Prints per-video processing time and average frame processing time.

# Main configuration:

videos_folder: path to the folder containing videos to process.

model_weights_path: path to the trained YOLO model weights.

imgsz: image size for YOLO detection (default 640).

# How to use it:

- Prepare a folder of videos to process.
- Ensure you have a trained YOLO model weights file.
- Update videos_folder and model_weights_path at the bottom of the script.
- Run the script with Python: "python video_analysis_with_tracker.py"
- Annotated videos will be automatically saved in the detected_videos_with_tracking_fine_tuneV2 folder alongside the source videos.
- At the end, the script prints total processing time and statistics per video.